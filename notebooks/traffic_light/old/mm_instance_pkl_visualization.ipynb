{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d3018fc-1725-478e-992b-e84196742087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import glob, cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import pycocotools.mask as mask\n",
    "\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import time\n",
    "import errno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8dc74d2-790c-4c2a-902d-0da07f52b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_cpkt_path = '/workspace/traffic_light/mmdetection/outputs_result/36_epoch_convnext2.pkl'\n",
    "test_image_path = \"/workspace/traffic_light/data/segmentation_coco/images/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f687ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=[\n",
    "    'Car_VehLane',\n",
    "    'Car_VehLane_IncatLft',\n",
    "    'Car_VehLane_IncatRht',\n",
    "    'Car_VehLane_HazLit',\n",
    "    'Car_VehLane_Brake',\n",
    "    'Car_VehLane_Brake_IncatLft',\n",
    "    'Car_VehLane_Brake_IncatRht',\n",
    "    'Car_VehLane_Brake_HazLit',\n",
    "    'Car_OutgoLane',\n",
    "    'Car_OutgoLane_IncatLft',\n",
    "    'Car_OutgoLane_IncatRht',\n",
    "    'Car_OutgoLane_HazLit',\n",
    "    'Car_OutgoLane_Brake',\n",
    "    'Car_OutgoLane_Brake_IncatLft',\n",
    "    'Car_OutgoLane_Brake_IncatRht',\n",
    "    'Car_OutgoLane_Brake_HazLit',\n",
    "    'Car_IncomLane',\n",
    "    'Car_IncomLane_IncatLft',\n",
    "    'Car_IncomLane_IncatRht',\n",
    "    'Car_IncomLane_HazLit',\n",
    "    'Car_IncomLane_Brake',\n",
    "    'Car_IncomLane_Brake_IncatLft',\n",
    "    'Car_IncomLane_Brake_IncatRht',\n",
    "    'Car_IncomLane_Brake_HazLit',\n",
    "    'Car_Jun',\n",
    "    'Car_Jun_IncatLft',\n",
    "    'Car_Jun_IncatRht',\n",
    "    'Car_Jun_HazLit',\n",
    "    'Car_Jun_Brake',\n",
    "    'Car_Jun_Brake_IncatLft',\n",
    "    'Car_Jun_Brake_IncatRht',\n",
    "    'Car_Jun_Brake_HazLit',\n",
    "    'Car_Parking',\n",
    "    'Car_Parking_IncatLft',\n",
    "    'Car_Parking_IncatRht',\n",
    "    'Car_Parking_HazLit',\n",
    "    'Car_Parking_Brake',\n",
    "    'Car_Parking_Brake_IncatLft',\n",
    "    'Car_Parking_Brake_IncatRht',\n",
    "    'Car_Parking_Brake_HazLit',\n",
    "    'Bus_VehLane',\n",
    "    'Bus_VehLane_IncatLft',\n",
    "    'Bus_VehLane_IncatRht',\n",
    "    'Bus_VehLane_HazLit',\n",
    "    'Bus_VehLane_Brake',\n",
    "    'Bus_VehLane_Brake_IncatLft',\n",
    "    'Bus_VehLane_Brake_IncatRht',\n",
    "    'Bus_VehLane_Brake_HazLit',\n",
    "    'Bus_OutgoLane',\n",
    "    'Bus_OutgoLane_IncatLft',\n",
    "    'Bus_OutgoLane_IncatRht',\n",
    "    'Bus_OutgoLane_HazLit',\n",
    "    'Bus_OutgoLane_Brake',\n",
    "    'Bus_OutgoLane_Brake_IncatLft',\n",
    "    'Bus_OutgoLane_Brake_IncatRht',\n",
    "    'Bus_OutgoLane_Brake_HazLit',\n",
    "    'Bus_IncomLane',\n",
    "    'Bus_IncomLane_IncatLft',\n",
    "    'Bus_IncomLane_IncatRht',\n",
    "    'Bus_IncomLane_HazLit',\n",
    "    'Bus_IncomLane_Brake',\n",
    "    'Bus_IncomLane_Brake_IncatLft',\n",
    "    'Bus_IncomLane_Brake_IncatRht',\n",
    "    'Bus_IncomLane_Brake_HazLit',\n",
    "    'Bus_Jun',\n",
    "    'Bus_Jun_IncatLft',\n",
    "    'Bus_Jun_IncatRht',\n",
    "    'Bus_Jun_HazLit',\n",
    "    'Bus_Jun_Brake',\n",
    "    'Bus_Jun_Brake_IncatRht',\n",
    "    'Bus_Jun_Brake_IncatLft',\n",
    "    'Bus_Jun_Brake_HazLit',\n",
    "    'Bus_Parking',\n",
    "    'Bus_Parking_IncatLft',\n",
    "    'Bus_Parking_IncatRht',\n",
    "    'Bus_Parking_HazLit',\n",
    "    'Bus_Parking_Brake',\n",
    "    'Bus_Parking_Brake_IncatLft',\n",
    "    'Bus_Parking_Brake_IncatRht',\n",
    "    'Bus_Parking_Brake_HazLit'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b3d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category2class(labels:list):\n",
    "    cls=[]\n",
    "    loc=[]\n",
    "    actions=[]\n",
    "    for label in labels:\n",
    "        action=[0,0,0,0]\n",
    "        cls.append(label//40)\n",
    "        loc.append(label%40//8)\n",
    "        action[0]=label%40%8//4\n",
    "        action[label%40%8%4]=1\n",
    "        actions.append(action)\n",
    "    return cls, loc, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d342aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rle2mask(compactRLESegmentations):\n",
    "#     segmentations=[]\n",
    "#     for compactRLESegmentation in compactRLESegmentations:\n",
    "#         maskedArr = mask.decode(compactRLESegmentation)\n",
    "#         binary_mask = maskedArr.astype('int')\n",
    "#         segmentations.append(binary_mask)\n",
    "#     return segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2bfb592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle2polygon(compactRLESegmentations):\n",
    "    segmentations=[]\n",
    "    for compactRLESegmentation in compactRLESegmentations:\n",
    "        maskedArr = mask.decode(compactRLESegmentation)\n",
    "        # area = float((maskedArr > 0.0).sum())\n",
    "        # adapted from https://github.com/hazirbas/coco-json-converter/blob/master/generate_coco_json.py\n",
    "        contours, _ = cv2.findContours(maskedArr, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        segmentation = []\n",
    "        valid_poly = 0\n",
    "        for contour in contours:\n",
    "        # Valid polygons have >= 6 coordinates (3 points)\n",
    "            if contour.size >= 6:\n",
    "                segmentation.append(contour.astype(float).flatten().tolist())\n",
    "                valid_poly += 1\n",
    "        if valid_poly == 0:\n",
    "            segmentation.append([-1]) # -1 means no poly\n",
    "            # raise ValueError\n",
    "        segmentations.extend(segmentation)\n",
    "        # areas.append(area)\n",
    "        \n",
    "    return segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e23ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle2polygon_mask(compactRLESegmentations):\n",
    "    segmentations=[]\n",
    "    masks=[]\n",
    "    for compactRLESegmentation in compactRLESegmentations:\n",
    "        maskedArr = mask.decode(compactRLESegmentation)\n",
    "        binary_mask = maskedArr.astype('int')\n",
    "        # area = float((maskedArr > 0.0).sum())\n",
    "        # adapted from https://github.com/hazirbas/coco-json-converter/blob/master/generate_coco_json.py\n",
    "        contours, _ = cv2.findContours(maskedArr, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        segmentation = []\n",
    "        valid_poly = 0\n",
    "        for contour in contours:\n",
    "        # Valid polygons have >= 6 coordinates (3 points)\n",
    "            if contour.size >= 6:\n",
    "                segmentation.append(contour.astype(float).flatten().tolist())\n",
    "                masks.append(binary_mask)\n",
    "                valid_poly += 1\n",
    "        if valid_poly == 0:\n",
    "            segmentation.append([-1]) # -1 means no poly\n",
    "            masks.append([[-1]])\n",
    "            # raise ValueError\n",
    "        segmentations.extend(segmentation)\n",
    "        # areas.append(area)\n",
    "        \n",
    "    return segmentations, masks #, area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa9224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xyxyn(polygons,w,h):\n",
    "    return [ float(e/w) if i%2==0 else float(e/h) for polygon in polygons for i, e in enumerate(polygon)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f17829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------Class Param------------\n",
    "agent_classes = [\"Car\", \"Bus\"]\n",
    "loc_classes = [\"VehLane\", \"OutgoLane\", \"IncomLane\", \"Jun\", \"Parking\"]\n",
    "action_classes = [\"Brake\", \"IncatLft\", \"IncatRht\", \"HazLit\"]\n",
    "class_nums = [len(agent_classes), len(loc_classes), len(action_classes)]\n",
    "# --------------Class Param------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1166b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "icons = {}\n",
    "\n",
    "for actions in action_classes:\n",
    "    target = \"../../ultra/Icons/\" + actions + \".png\"\n",
    "    icon_img = cv2.imread(target)\n",
    "    icon_img = cv2.cvtColor(icon_img, cv2.COLOR_BGR2RGB)\n",
    "    icons[actions] = icon_img\n",
    "\n",
    "for actions in loc_classes:\n",
    "    # print(actions)\n",
    "    target = \"../../ultra/Icons/\" + actions + \".png\"\n",
    "    icon_img = cv2.imread(target)\n",
    "    icon_img = cv2.cvtColor(icon_img, cv2.COLOR_BGR2RGB)\n",
    "    icons[actions] = icon_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73d02bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seg_plot_one_box(x, idx, img, mask, cls, loc, action, color=None, label=None, track_id=None, line_thickness=None):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or 2  # line thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    \n",
    "    c1, c2 = (\n",
    "        (np.clip(int(x[0]), 0, img.shape[1]), np.clip(int(x[1]), 0, img.shape[0])),\n",
    "        (np.clip(int(x[2]), 0, img.shape[1]), np.clip(int(x[3]), 0, img.shape[0])),\n",
    "    )\n",
    "\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=1)\n",
    "\n",
    "    agent_list = [\"Car\", \"Bus\"]\n",
    "    loc_list = [\"VehLane\", \"OutgoLane\", \"IncomLane\", \"Jun\", \"Parking\"]\n",
    "    action_list = [\"Brake\", \"IncatLft\", \"IncatRht\", \"HazLit\"]\n",
    "\n",
    "    num_icon = np.sum(action)\n",
    "\n",
    "    icon_size = int(np.min([(c2[0] - c1[0]) / num_icon, (x[3] - x[1]) / 2, 64]))\n",
    "    c3 = c1[0]  # +(c2[0]-c1[0])//2-icon_size*num_icon//2\n",
    "\n",
    "    try:\n",
    "        offset_icon = 0\n",
    "        for ii in range(len(action)):\n",
    "            if action[ii] == 1:\n",
    "                img[c1[1] : c1[1] + icon_size, c3 + offset_icon : c3 + offset_icon + icon_size, :] = (\n",
    "                    cv2.resize(icons[action_list[ii]], (icon_size, icon_size), interpolation=cv2.INTER_NEAREST) * 0.5\n",
    "                    + img[c1[1] : c1[1] + icon_size, c3 + offset_icon : c3 + offset_icon + icon_size, :] * 0.5\n",
    "                )\n",
    "                offset_icon += icon_size\n",
    "\n",
    "        img[c2[1] - icon_size : c2[1], c3 : c3 + icon_size, :] = (\n",
    "            cv2.resize(icons[loc_list[loc]], (icon_size, icon_size)) * 0.5\n",
    "            + img[c2[1] - icon_size : c2[1], c3 : c3 + icon_size, :] * 0.5\n",
    "        )\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Expand mask dimensions to match the image\n",
    "    mask = mask[c1[1] : c2[1], c1[0] : c2[0]]\n",
    "    mask = mask > 0.5\n",
    "\n",
    "    img[c1[1] : c2[1], c1[0] : c2[0], :][mask] = (\n",
    "        img[c1[1] : c2[1], c1[0] : c2[0], :][mask] * 0.65 + np.array(color) * 0.35\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd197023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed New dir\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import time\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(\"/workspace/traffic_light/ultra/Result/segment/predictions/v2/img\")\n",
    "    print(\"Removed New dir\")\n",
    "except:\n",
    "    print(\"Making New dir\")\n",
    "\n",
    "filepath = \"/workspace/traffic_light/ultra/Result/segment/predictions/v2/img\"\n",
    "if not os.path.exists(filepath):\n",
    "    try:\n",
    "        os.makedirs(filepath)\n",
    "    except OSError as exc:  # Guard against race condition\n",
    "        if exc.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "COLORS = [\n",
    "    [255, 0, 0],  # Red\n",
    "    [0, 255, 0],  # Green\n",
    "    [0, 0, 255],  # Blue\n",
    "    [255, 255, 0],  # Yellow\n",
    "    [255, 0, 255],  # Magenta\n",
    "    [0, 255, 255],  # Cyan\n",
    "    [128, 0, 0],  # Maroon\n",
    "    [0, 128, 0],  # Green (dark)\n",
    "    [0, 0, 128],  # Navy\n",
    "    [128, 128, 0],  # Olive\n",
    "    [128, 0, 128],  # Purple\n",
    "    [0, 128, 128],  # Teal\n",
    "    [255, 165, 0],  # Orange\n",
    "    [210, 180, 140],  # Tan\n",
    "    [255, 192, 203],  # Pink\n",
    "    [0, 128, 128],  # Teal\n",
    "    [255, 99, 71],  # Tomato\n",
    "    [139, 69, 19],  # Saddle Brown\n",
    "    [0, 128, 0],  # Green (dark)\n",
    "    [255, 20, 147],  # Deep Pink\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ddb42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(bounding_boxes, confidence_score, labels, locations, actions, polygons, threshold):\n",
    "    \"\"\"\n",
    "    ref: https://github.com/amusi/Non-Maximum-Suppression/blob/master/nms.py\n",
    "    the boxes format is xyxy\n",
    "    e.g) \n",
    "    bounding_boxes = [(187, 82, 337, 317), (150, 67, 305, 282), (246, 121, 368, 304)]\n",
    "    confidence_score = [0.9, 0.75, 0.8]\n",
    "    threshold = 0.4\n",
    "    \"\"\"\n",
    "    # If no bounding boxes, return empty list\n",
    "    if len(bounding_boxes) == 0:\n",
    "        return [], []\n",
    "\n",
    "    # Bounding boxes\n",
    "    boxes = np.array(bounding_boxes)\n",
    "\n",
    "    # coordinates of bounding boxes\n",
    "    start_x = boxes[:, 0]\n",
    "    start_y = boxes[:, 1]\n",
    "    end_x = boxes[:, 2]\n",
    "    end_y = boxes[:, 3]\n",
    "\n",
    "    # Confidence scores of bounding boxes\n",
    "    score = np.array(confidence_score)\n",
    "    \n",
    "    # print(boxes.shape[0])\n",
    "    # print(score.shape[0])\n",
    "    \n",
    "    # variables = [boxes.shape[0], score.shape[0], len(labels), len(locations), len(actions), len(polygons)]\n",
    "    # reference = variables[0]\n",
    "    # different_values = [value for value in variables if value != reference]\n",
    "\n",
    "    # if different_values:\n",
    "    #     print(\"Different values found:\", different_values)\n",
    "    # else:\n",
    "    #     print(\"All values are the same.\")\n",
    "        \n",
    "    # Picked bounding boxes\n",
    "    picked_boxes = []\n",
    "    picked_score = []\n",
    "    picked_labels = []\n",
    "    picked_locations=[]\n",
    "    picked_actions=[]\n",
    "    picked_polygons = []\n",
    "\n",
    "    # Compute areas of bounding boxes\n",
    "    areas = (end_x - start_x + 1) * (end_y - start_y + 1)\n",
    "\n",
    "    # Sort by confidence score of bounding boxes\n",
    "    order = np.argsort(score)\n",
    "\n",
    "    # Iterate bounding boxes\n",
    "    while order.size > 0:\n",
    "        # The index of largest confidence score\n",
    "        index = order[-1]\n",
    "\n",
    "        # Pick the bounding box with largest confidence score\n",
    "        picked_boxes.append(bounding_boxes[index])\n",
    "        picked_score.append(confidence_score[index])\n",
    "        picked_labels.append(labels[index])\n",
    "        picked_locations.append(locations[index])\n",
    "        picked_actions.append(actions[index])\n",
    "        picked_polygons.append(polygons[index])\n",
    "\n",
    "        # Compute ordinates of intersection-over-union(IOU)\n",
    "        x1 = np.maximum(start_x[index], start_x[order[:-1]])\n",
    "        x2 = np.minimum(end_x[index], end_x[order[:-1]])\n",
    "        y1 = np.maximum(start_y[index], start_y[order[:-1]])\n",
    "        y2 = np.minimum(end_y[index], end_y[order[:-1]])\n",
    "\n",
    "        # Compute areas of intersection-over-union\n",
    "        w = np.maximum(0.0, x2 - x1 + 1)\n",
    "        h = np.maximum(0.0, y2 - y1 + 1)\n",
    "        intersection = w * h\n",
    "\n",
    "        # Compute the ratio between intersection and union\n",
    "        ratio = intersection / (areas[index] + areas[order[:-1]] - intersection)\n",
    "\n",
    "        left = np.where(ratio < threshold)\n",
    "        order = order[left]\n",
    "    \n",
    "    # if not all(picked_labels, picked_locations, picked_actions, picked_polygons):\n",
    "    #     return (False, False, False, False, False, False)\n",
    "    return picked_boxes, picked_score, picked_labels, picked_locations, picked_actions, picked_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5b2e7-4ed5-4d7d-a840-39e8081c3c32",
   "metadata": {},
   "source": [
    "# visualization\n",
    "- ref : https://huggingface.co/docs/transformers/tasks/object_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45622ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8909/8909 [03:34<00:00, 41.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty_files:  ['Banseok02_Snow_00028957', 'CounterClock_Fog_00040121', 'Banseok02_Snow_00029012', 'Ulsan01_Day_00018554', 'CounterClock_Fog_00040136', 'CounterClock_Fog_00040076', 'OiSam_Day_00020123', 'Ulsan01_Day_00018549', 'OiSam_Day_00020151', 'Banseok02_Snow_00028977', 'CounterClock_Fog_00040101', 'Banseok02_Snow_00029002', 'CounterClock_Fog_00040146', 'Banseok02_Snow_00029017', 'OiSam_Day_00020159', 'Ulsan01_Day_00018559', 'Banseok02_Snow_00028982', 'CounterClock_Fog_00040016', 'CounterClock_Fog_00040071', 'Banseok02_Snow_00028932', 'Banseok02_Snow_00025672', 'OiSam_Day_00020531', 'CounterClock_Fog_00040111', 'CounterClock_Fog_00040106', 'CounterClock_Fog_00040126', 'CounterClock_Fog_00040081', 'OiSam_Day_00020131', 'Banseok02_Snow_00028972', 'Banseok02_Snow_00028952', 'Banseok02_Snow_00028962', 'Banseok02_Snow_00028987', 'CounterClock_Fog_00040141', 'CounterClock_Fog_00040011', 'Banseok02_Snow_00028992', 'Ulsan01_Day_00018564', 'Banseok02_Snow_00028927', 'CounterClock_Fog_00040151', 'CounterClock_Fog_00040091', 'OiSam_Day_00020143', 'CounterClock_Fog_00040086', 'OiSam_Day_00020139', 'OiSam_Day_00020135', 'KCITY02_Snow_00002267', 'CounterClock_Fog_00040116', 'Banseok02_Snow_00028947', 'Banseok02_Snow_00029007', 'Banseok02_Snow_00028967', 'Banseok02_Snow_00028942', 'OiSam_Day_00020127', 'OiSam_Day_00020147', 'CounterClock_Fog_00040096', 'OiSam_Day_00020155', 'Banseok02_Snow_00028922', 'Banseok02_Snow_00028997', 'CounterClock_Fog_00040131']\n",
      "empty_files_cnt:  55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nms_iou_threshold=0.6\n",
    "inference_img=False\n",
    "inference_txt=True\n",
    "\n",
    "input_base_dir = '/workspace/traffic_light/data/segmentation_coco/images/test'\n",
    "# target_folder = \"/workspace/traffic_light/data/segmentation/test/IllegalParking01_Rain/img\"  # Target Dir\n",
    "\n",
    "output_base_dir=\"/workspace/traffic_light/seg_output3\"\n",
    "\n",
    "if len(glob.glob(os.path.join(input_base_dir, \"*.png\"))) < 100:\n",
    "    print(\"Error : Directory Invalid\")\n",
    "    sys.exit(0)\n",
    "\n",
    "cnt=1\n",
    "\n",
    "empty_files=[]\n",
    "empty_files_cnt=0\n",
    "with open(dino_cpkt_path, 'rb') as f:\n",
    "    datas = pickle.load(f)\n",
    "    for index, data in enumerate(tqdm(datas)):\n",
    "        _name=os.path.splitext(os.path.basename(data['img_path']))[0]\n",
    "\n",
    "        # Extract city name and file name\n",
    "        splited_name=_name.split('_')\n",
    "        city_name = '_'.join(splited_name[:2])\n",
    "        num_name = splited_name[2]\n",
    "        if city_name =='Sejong_L1':\n",
    "            city_name = 'Sejong_L1_00'\n",
    "            num_name=splited_name[3]\n",
    "        # print(city_name, type(city_name))\n",
    "        # if city_name!=\"Sejong_L1_00\":\n",
    "        #     continue\n",
    "        # Define the output text file path\n",
    "        output_dir = os.path.join(output_base_dir, city_name, 'txt')\n",
    "        output_file = os.path.join(output_dir, f\"{num_name}.txt\")\n",
    "        \n",
    "        # Create the output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    " \n",
    "        # if not os.path.exists(output_file):\n",
    "        #     with open(output_file, 'w') as f3:\n",
    "        #         pass\n",
    "        # continue\n",
    "        classes, locations, actions=category2class(data['pred_instances']['labels'].tolist())\n",
    "\n",
    "        if not classes:\n",
    "            # print('empty file: ', _name)\n",
    "            with open(output_file, \"w\") as f2:\n",
    "                pass\n",
    "            empty_files_cnt+=1\n",
    "            empty_files.append(_name)\n",
    "            continue\n",
    "        \n",
    "        xyxy = data['pred_instances']['bboxes']\n",
    "        scores=data['pred_instances']['scores']\n",
    "        boxes=data['pred_instances']['bboxes'].numpy()\n",
    "        ori_h, ori_w=datas[0]['ori_shape']\n",
    "\n",
    "        compactRLESegmentation = data['pred_instances']['masks']\n",
    "        # masks = rle2mask(compactRLESegmentation)\n",
    "        if inference_img:\n",
    "            polygons, masks = rle2polygon_mask(compactRLESegmentation)\n",
    "        else:\n",
    "            polygons=rle2polygon(compactRLESegmentation)\n",
    "\n",
    "        # apply NMS\n",
    "        if nms_iou_threshold != 1:\n",
    "            _, scores, classes, locations, actions, polygons=nms(boxes, scores, classes, locations, actions, polygons, nms_iou_threshold)\n",
    "        \n",
    "        if inference_img:\n",
    "            target_img = np.array(Image.open(data['img_path']).convert(\"RGB\"))\n",
    "            for i in range(len(scores)):\n",
    "                if masks[i][0][0] == -1 or (not classes):\n",
    "                    continue\n",
    "                seg_plot_one_box(\n",
    "                    xyxy[i],\n",
    "                    i,\n",
    "                    target_img,\n",
    "                    np.array(masks[i]),\n",
    "                    classes[i],\n",
    "                    locations[i],\n",
    "                    actions[i],\n",
    "                    color=COLORS[i % len(COLORS)],\n",
    "                )\n",
    "\n",
    "            target_img = target_img.copy()\n",
    "\n",
    "            path = \"/workspace/traffic_light/ultra/Result/segment/predictions/v2/img/\" + _name + \".png\"\n",
    "            cv2.imwrite(path, target_img[:, :, ::-1])\n",
    "        # else:\n",
    "            # target_img = np.array(Image.open(data['img_path']).convert(\"RGB\"))\n",
    "        #     for i in range(len(picked_score)):\n",
    "        #         if masks[i][0][0] == -1 or (not picked_labels):\n",
    "        #             continue\n",
    "        #         seg_plot_one_box(\n",
    "        #             xyxy[i],\n",
    "        #             i,\n",
    "        #             target_img,\n",
    "        #             np.array(masks[i]),\n",
    "        #             picked_labels[i],\n",
    "        #             picked_locations[i],\n",
    "        #             picked_actions[i],\n",
    "        #             color=COLORS[i % len(COLORS)],\n",
    "        #         )\n",
    "            \n",
    "        #     print(_name)\n",
    "        #     plt.rcParams[\"figure.figsize\"] = [20, 10]\n",
    "        #     plt.imshow(target_img)\n",
    "            \n",
    "        if inference_txt:\n",
    "            with open(output_file, \"w\") as f:\n",
    "                for score, cls, loc, action, poly in zip(scores, classes, locations, actions, polygons):\n",
    "                    poly=xyxy2xyxyn(polygons, ori_w, ori_h)\n",
    "                    score=score.item()\n",
    "                    if (not cls) and len(poly) > 5 and poly[0] > -1 and score > 0.25:\n",
    "\n",
    "                        result_txt = (\n",
    "                            str(score) # cs\n",
    "                            + \" \"\n",
    "                            + str(cls)\n",
    "                            + \" \"\n",
    "                            + str(loc)\n",
    "                            + \" \"\n",
    "                            + \" \".join(map(str, action))\n",
    "                            + \" \"\n",
    "                            + \" \".join(map(str, poly)) # polygon\n",
    "                            + \"\\n\"\n",
    "                        )\n",
    "                        f.write(result_txt)\n",
    "                    \n",
    "\n",
    "        \n",
    "        # if index >1:\n",
    "        #     break\n",
    "# print('cnt',cnt)\n",
    "print('empty_files: ', empty_files)\n",
    "print('empty_files_cnt: ',empty_files_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e8050677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doryong_Cons   0\n",
      "Ulsan01_Day   0\n",
      "Yongsan_Cons   0\n",
      "IllegalParking04_Rain   0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/traffic_light/data/segmentation/test/Sejong_L1/img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/traffic_light/seg_output\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      3\u001b[0m     test_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/traffic_light/data/segmentation/test/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/img\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     test_list \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(test_dir)\n\u001b[1;32m      5\u001b[0m     out_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/workspace/traffic_light/seg_output/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m     cnt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/workspace/traffic_light/data/segmentation/test/Sejong_L1/img'"
     ]
    }
   ],
   "source": [
    "for folder_name in os.listdir('/workspace/traffic_light/seg_output'):\n",
    "    \n",
    "    test_dir=f'/workspace/traffic_light/data/segmentation/test/{folder_name}/img'\n",
    "    test_list = os.listdir(test_dir)\n",
    "    out_path=f'/workspace/traffic_light/seg_output/{folder_name}/txt'\n",
    "    cnt=0\n",
    "    for tl in test_list:\n",
    "        name=tl.split('.')[0]\n",
    "        # print(name)\n",
    "        if not os.path.exists(os.path.join(out_path,f'{name}.txt')):\n",
    "            cnt+=1\n",
    "            with open(os.path.join(out_path,f'{name}.txt'), 'w') as f3:\n",
    "                pass\n",
    "    print(folder_name,' ',cnt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
